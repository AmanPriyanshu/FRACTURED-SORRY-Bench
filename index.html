<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>FRACTURED-SORRY-Bench</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f0f0f0;
        }
        header {
            background-color: #2c3e50;
            color: white;
            text-align: center;
            padding: 1em;
            margin-bottom: 2em;
        }
        h1 {
            margin: 0;
            font-size: 2.5em;
        }
        .subtitle {
            font-style: italic;
            margin-top: 0.5em;
        }
        .highlight {
            background-color: #e74c3c;
            color: white;
            padding: 0.2em 0.5em;
            border-radius: 3px;
        }
        .section {
            background-color: white;
            padding: 2em;
            margin-bottom: 2em;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .button {
            display: inline-block;
            padding: 10px 20px;
            background-color: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 5px;
            transition: background-color 0.3s;
        }
        .button:hover {
            background-color: #2980b9;
        }
    </style>
</head>
<body>
    <header>
        <h1>FRACTURED-SORRY-Bench</h1>
        <p class="subtitle">Framework for Revealing Attacks in Conversational Turns Undermining Refusal Efficacy and Defenses over SORRY-Bench</p>
    </header>

    <div class="section">
        <h2>About FRACTURED-SORRY-Bench</h2>
        <p>FRACTURED-SORRY-Bench is a novel framework for evaluating the safety of Large Language Models (LLMs) against <span class="highlight">multi-turn conversational attacks</span>. Building upon the SORRY-Bench dataset, we propose a simple yet effective method for generating adversarial prompts by breaking down harmful queries into seemingly innocuous sub-questions.</p>
    </div>

    <div class="section">
        <h2>Key Features</h2>
        <ul>
            <li>Extends SORRY-Bench with multi-turn attack scenarios</li>
            <li>Provides a <span class="highlight">simple and efficient</span> method for generating adversarial samples</li>
            <li>Evaluates LLM safety against subtle, conversation-based attacks</li>
            <li>Achieves significant increases in Attack Success Rates (ASRs)</li>
        </ul>
    </div>

    <div class="section">
        <h2>Results Preview</h2>
        <p>Our approach achieves a maximum increase of X% in Attack Success Rates (ASRs) across GPT-4, GPT-4o, GPT-4o-mini, and GPT-3.5-Turbo models compared to baseline methods.</p>
    </div>

    <div class="section">
        <h2>Get Started</h2>
        <a href="https://github.com/AmanPriyanshu/FRACTURED-SORRY-Bench/" class="button">View on GitHub</a>
        <a href="#" class="button">Read the Paper</a>
    </div>

    <footer>
        <p>&copy; 2024 FRACTURED-SORRY-Bench Team. All rights reserved.</p>
    </footer>
</body>
</html>